# AI Capabilities in CRM
Exam Weight 8%

## Responsible Creation of Artificial Intelligence
Design e desenvolvimento de tecnologia responsável
Estamos apenas começando a entender como a tecnologia emergente impacta a sociedade. Diversas questões surgiram, desde questões sobre a automação substituindo empregos até especulações sobre os efeitos desenvolvimentistas da mídia social.  

Muitas indústrias são governadas por padrões, protocolos e regulamentos destinados a garantir que seus produtos tenham um impacto positivo na sociedade. Médicos, por exemplo, seguem o Juramento de Hipócrates e estabeleceram conselhos de revisão de pesquisa para garantir práticas éticas. A indústria automobilística está sujeita às leis de direção e aos padrões de segurança em torno de cintos de segurança e airbags. De forma mais geral, em 2011, as Nações Unidas endossaram os Princípios Orientadores para Empresas e Direitos Humanos , que definem as responsabilidades que empresas e estados têm para proteger os direitos e liberdades concedidos a todos os indivíduos.

Na Salesforce, entendemos que temos uma responsabilidade mais ampla com a sociedade e aspiramos criar tecnologia que não apenas impulsione o sucesso de nossos clientes, mas também impulsione mudanças sociais positivas e beneficie a humanidade. A Salesforce estabeleceu o Office of Ethical and Humane Use para abrir caminho na Quarta Revolução Industrial, ajudando nossos clientes a usar nossa tecnologia para causar um impacto positivo. Esse esforço está ancorado nos valores essenciais da Salesforce (confiança, sucesso do cliente, inovação, igualdade e sustentabilidade).

Valores essenciais da Salesforce estampados em um escudo: confiança, sucesso do cliente, inovação, igualdade e sustentabilidade
Quando se trata de ética tecnológica, as questões nunca foram tão urgentes — e cabe a todos nós encontrar as soluções.

O que significa ser tendencioso ou justo?
Ao criar ou usar tecnologia, especialmente envolvendo inteligência artificial ou automação, é importante se questionar sobre parcialidade e imparcialidade.

Na Salesforce, vemos o viés como "erros sistemáticos e repetíveis em um sistema de computador que criam resultados injustos, de maneiras diferentes da função pretendida do sistema, devido a suposições imprecisas no processo de aprendizado de máquina". No contexto das estatísticas, o viés é o desvio sistemático da verdade ou erro. De uma perspectiva social e legal, a pesquisadora e professora Kate Crawford define o viés como "julgamento baseado em noções preconcebidas ou preconceitos, em oposição à avaliação imparcial dos fatos". 

Justiça é definida como uma decisão tomada livre de interesse próprio, preconceito ou favoritismo. Na realidade, é quase impossível que uma decisão seja perfeitamente justa. Um painel na Conferência sobre Justiça, Responsabilidade e Transparência da Association for Computing Machinery em 2018 desenvolveu uma lista de mais de 21 definições de justiça . Se há tantas maneiras de pensar sobre justiça, como você pode dizer se humanos ou máquinas estão tomando decisões justas? 

Três pessoas imaginando maneiras diferentes, iguais e justas de cortar um bolo.

Para tomar uma decisão mais informada, é fundamental entender o impacto dessa decisão. Uma decisão que beneficia o maior número de pessoas ainda exclui uma minoria, o que é injusto se essa minoria é frequentemente ignorada. Você precisa se perguntar: Alguns indivíduos ou grupos são desproporcionalmente impactados por uma decisão? O viés sistêmico em decisões passadas ou dados imprecisos tornam alguns grupos menos propensos a receber uma avaliação justa ou imparcial? Se a resposta for sim, então você deve decidir se, e como, você deve otimizar para proteger esses indivíduos, mesmo que isso não beneficie a maioria.

Existe algo como preconceito "bom"?
Alguns podem argumentar que nem todos os vieses são ruins. Por exemplo, digamos que uma empresa farmacêutica fabrique um medicamento para câncer de próstata e tenha como alvo apenas homens em suas campanhas de marketing. A empresa acredita que a segmentação de sua campanha de marketing é um exemplo de um bom viés porque está prestando um serviço ao não incomodar o público feminino com anúncios irrelevantes. Mas se o conjunto de dados da empresa incluísse apenas pessoas cisgênero ou não reconhecesse identidades adicionais (por exemplo, não binários, mulheres transgênero, homens transgênero e indivíduos agênero), então eles provavelmente estavam excluindo outras pessoas que se beneficiariam ao ver o anúncio. Ao incorporar uma compreensão mais complexa e precisa de gênero e identidade de gênero, a empresa estaria melhor equipada para atingir todos que poderiam se beneficiar do medicamento.

Crie uma cultura ética
A maioria das empresas não se propõe ativamente a ofender ou prejudicar as pessoas. Mas elas podem fazer isso involuntariamente se não definirem seus valores essenciais e colocarem processos em prática para garantir que todos na empresa estejam trabalhando de acordo com eles. Ao definir valores, processos e incentivos, os líderes podem influenciar a cultura em suas empresas. Os líderes podem e devem ensinar alunos e funcionários a aplicar a ética em seu trabalho, mas se a cultura da empresa não for saudável, é como plantar uma árvore saudável em um pomar estragado. Eventualmente, até mesmo a árvore saudável produz maçãs podres. Os líderes devem recompensar o comportamento ético enquanto capturam e interrompem o comportamento antiético. 

É importante lembrar que todos nós somos líderes neste domínio. Você pode fazer a diferença introduzindo e mantendo uma cultura ética com uma abordagem de ponta a ponta. 

Crie equipes diversificadas.
Traduzir valores em processos.
Entenda seus clientes.
Crie equipes diversas
Pesquisas mostram que equipes diversas (em todo o espectro de experiência, raça, gênero e habilidade) são mais criativas, diligentes e trabalhadoras. Uma organização que inclui mais mulheres em todos os níveis, especialmente na alta gerência, normalmente tem lucros maiores. Para saber mais, confira a seção Recursos no final desta unidade. 

Onze pessoas com diversas habilidades, raças, gêneros e idades.

Tudo o que criamos representa nossos valores, experiências e preconceitos. Por exemplo, sistemas de reconhecimento facial geralmente têm mais dificuldade em identificar rostos negros ou pardos do que rostos brancos. Se as equipes que criam essa tecnologia fossem mais diversas, elas teriam mais probabilidade de reconhecer e abordar esse preconceito. 

As equipes de desenvolvimento devem se esforçar para atingir a diversidade em todas as áreas, desde idade e raça até cultura, educação e habilidade. A falta de diversidade pode criar uma câmara de eco que resulta em produtos tendenciosos e lacunas de recursos. Se você não puder contratar membros de equipe diversos, considere buscar feedback de grupos sub-representados em sua empresa e base de usuários.

Um senso de comunidade também faz parte da base ética de uma empresa. Nenhuma pessoa deve ser a única responsável por agir eticamente ou promover a ética. Em vez disso, a empresa como um todo deve estar atenta e consciente da ética. Os funcionários devem se sentir confortáveis ​​desafiando o status quo e falando abertamente, o que pode identificar riscos para o seu negócio. Os membros da equipe devem fazer perguntas éticas específicas para seus domínios, como: 

Gerentes de produto : Qual é o impacto comercial de um falso positivo ou falso negativo em nosso algoritmo?
Pesquisadores : Quem é impactado pelo nosso sistema e como? Como ele pode ser abusado? Como as pessoas podem tentar quebrar o produto ou usá-lo de maneiras não intencionais? Qual é o contexto social em que isso é usado?
Designers : Quais padrões ou suposições estou construindo no produto? Estou projetando isso para transparência e igualdade?
Cientistas de dados : Quais são as implicações para os usuários quando otimizo meu modelo dessa maneira?
Redatores de conteúdo : Posso explicar por que o sistema fez uma previsão, recomendação ou decisão em termos que o usuário possa entender?
Engenheiros : Quais notificações, processos, verificações ou medidas de segurança podemos incorporar ao sistema para mitigar danos?
Funcionários fazendo perguntas éticas específicas para suas funções.

Observe que essas perguntas envolvem as perspectivas de múltiplas funções. Envolver stakeholders e membros da equipe em cada estágio do ciclo de vida do desenvolvimento do produto ajuda a corrigir o impacto das desigualdades sociais sistêmicas em seu sistema. Se você se encontrar em uma equipe que não tem nenhuma dessas funções, ou onde você desempenha múltiplas funções, pode ser necessário usar vários chapéus para garantir que cada uma dessas perspectivas seja incluída — e isso pode envolver buscar expertise ou aconselhamento externo. Quando os funcionários estão insatisfeitos com as respostas que recebem, deve haver um processo claro para resolver as áreas problemáticas, como um conselho de revisão. Entraremos em mais detalhes sobre isso mais tarde. 

Traduzir valores em processos
Quase toda organização tem um conjunto de valores projetados para orientar a tomada de decisão de seus funcionários. Há três maneiras de colocar isso em prática.

Estruturas de incentivos
Recursos
Documentação e Comunicação
Estruturas de incentivos

Estruturas de incentivo recompensam indivíduos por comportamentos específicos ou por atingirem objetivos específicos. Estruturas de incentivo devem ser informadas por valores organizacionais. Mais frequentemente do que não, funcionários são recompensados ​​com base em vendas, aquisição de clientes e engajamento do usuário. Essas métricas podem, às vezes, ir contra a tomada de decisão ética. 

Se uma organização deseja recompensar comportamentos alinhados com seus valores, possíveis estruturas de incentivo podem incluir recompensas éticas. Semelhante às recompensas por bugs, as recompensas éticas recompensam os funcionários quando eles identificam decisões, processos ou recursos que são contrários aos valores da empresa ou causam danos a outros ou recompensam representantes de vendas que compartilham preocupações sobre negócios com clientes e evitam riscos potenciais legais ou de relações públicas.

Você também pode incluir perguntas sobre desenvolvimento de tecnologia ética no seu processo de contratação. Isso define a expectativa para novos funcionários de que a cultura ética que você está construindo é importante para a empresa e que o pensamento e o comportamento éticos são recompensados.

Suporte ao Funcionário 

Organizações responsáveis ​​fornecem recursos para dar suporte aos funcionários e capacitá-los a tomar decisões alinhadas aos valores da empresa. Isso pode incluir educação dos funcionários (Trailhead é um ótimo recurso para isso) e conselhos de revisão para resolver problemas difíceis e garantir que os funcionários estejam seguindo as diretrizes. 

Na Salesforce, temos um conselho de revisão de ciência de dados, que fornece feedback sobre a qualidade e considerações éticas de nossos modelos de IA, dados de treinamento e aplicativos que os utilizam. 

Embora a construção de uma cultura ética capacite os funcionários a falar quando tiverem preocupações éticas, você também pode considerar a criação de um processo claro e anônimo para que os funcionários enviem preocupações. Por fim, as listas de verificação são ótimos recursos para provocar discussões e garantir que você não negligencie preocupações importantes. As listas de verificação, embora consistentes e fáceis de implementar, raramente são exaustivas e devem ser claras e acionáveis ​​para serem úteis. Como permitem que os funcionários tenham conversas difíceis, as listas de verificação ajudam sua empresa a construir uma cultura ética do zero. 

Lista de verificação em uma prancheta para mostrar a conclusão das ações.

Documentação e Comunicação

Documente a tomada de decisão para transparência e consistência. Se uma equipe estiver em uma encruzilhada ética, documentar o que é decidido e por que permite que equipes futuras aprendam com essa experiência e ajam consistentemente em vez de arbitrariamente. A documentação e a comunicação também dão aos seus funcionários e stakeholders confiança em seu processo e nas decisões resultantes.

Entenda seus clientes
Não é preciso dizer que você precisa entender todos os seus clientes. Se não fizer isso, você pode estar projetando produtos que ignoram uma parte da sua base de usuários ou causam danos a alguns usuários sem que você saiba. Pergunte a si mesmo, quais necessidades e valores você assumiu em vez de consultar? Quem corre maior risco de dano e por quê? Existem atores ruins que podem usar seu produto intencionalmente para causar danos ou indivíduos que podem usá-lo ignorantemente e causar danos acidentalmente? Depois de saber as respostas para essas perguntas, você pode trabalhar para resolver esses problemas. Recomendamos entrar em contato com um pesquisador de usuários para saber mais sobre seus clientes.
### Understand the Ethical Use of Technology
Foco em Inteligência Artificial
A inteligência artificial pode aumentar a inteligência humana, amplificar as capacidades humanas e fornecer insights acionáveis ​​que geram melhores resultados para nossos funcionários, clientes, parceiros e comunidades.

Acreditamos que os benefícios da IA ​​devem ser acessíveis a todos, não apenas aos criadores. Não basta entregar apenas a capacidade tecnológica da IA. Também temos a importante responsabilidade de garantir que nossos clientes possam usar nossa IA de forma segura e inclusiva para todos. Levamos essa responsabilidade a sério e estamos comprometidos em fornecer aos nossos funcionários, clientes, parceiros e comunidade as ferramentas de que precisam para desenvolver e usar a IA com segurança, precisão e ética. 

Como você aprende no distintivo Fundamentos de Inteligência Artificial , IA é um termo genérico que se refere a esforços para ensinar computadores a executar tarefas complexas e se comportar de maneiras que dão a aparência de agência humana. O treinamento para tal tarefa geralmente requer grandes quantidades de dados, permitindo que o computador aprenda padrões nos dados. Esses padrões formam um modelo que representa um sistema complexo, assim como você pode criar um modelo do nosso sistema solar. E com um bom modelo, você pode fazer boas previsões (como prever o próximo eclipse solar) ou gerar conteúdo (como escreva-me um poema escrito por um pirata).

Nem sempre sabemos por que um modelo está fazendo uma previsão específica ou gerando conteúdo de uma certa maneira. Frank Pasquale, autor de The Black Box Society , descreve essa falta de transparência como o fenômeno da caixa preta . Embora as empresas que criam IA possam explicar os processos por trás de seus sistemas, é mais difícil para elas dizer o que está acontecendo em tempo real e em que ordem, incluindo onde o viés pode estar presente no modelo. A IA apresenta desafios únicos quando se trata de viés e tomada de decisões justas.

O que é ético versus legal?
Toda sociedade tem leis que os cidadãos precisam cumprir. Às vezes, no entanto, você precisa pensar além da lei para desenvolver tecnologia ética. Por exemplo, a lei federal dos EUA protege certas características que você geralmente não pode usar em decisões envolvendo contratação, promoção, moradia, empréstimo ou assistência médica. Essas classes protegidas incluem sexo, raça, idade, deficiência, cor, nacionalidade, religião ou credo e informações genéticas. Se seus modelos de IA usam essas características, você pode estar infringindo a lei. Se seu modelo de IA está tomando uma decisão em que é legal confiar nessas características, ainda pode não ser ético permitir esses tipos de preconceitos. Problemas relacionados a classes protegidas também podem cruzar para o reino da privacidade e legalidade, então recomendamos seguir nossa trilha GDPR para saber mais . Por fim, também é importante estar ciente das maneiras pelas quais os produtos Einstein podem ou não ser usados ​​de acordo com nossa Política de Uso Aceitável .

A boa notícia é que a IA apresenta uma oportunidade de abordar sistematicamente o viés. Historicamente, se você reconhecesse que a tomada de decisão da sua empresa resultava em um resultado tendencioso como resultado da tomada de decisão individual, era difícil redesenhar todo o processo e superar esse viés intrínseco. Agora, com os sistemas de IA, temos a chance de incorporar justiça ao design e melhorar as práticas existentes.

Além de examinar cuidadosamente as implicações legais e éticas dos seus modelos de IA, você deve avaliar se seu modelo está alinhado com a responsabilidade da sua empresa de respeitar e promover os direitos humanos. Você deve levar em consideração a lei internacional de direitos humanos e as responsabilidades que a ONU estabeleceu para as empresas respeitarem os direitos humanos , que incluem um processo de due diligence para avaliar os impactos nos direitos humanos, agir sobre a avaliação e comunicar como os impactos são abordados. 

Tipos de preconceito a serem observados
O viés se manifesta de várias maneiras. Às vezes, é o resultado de erro sistemático. Outras vezes, é o resultado de preconceito social. Às vezes, a distinção é confusa. Com essas duas fontes de viés em mente, vamos analisar as maneiras pelas quais o viés pode entrar em um sistema de IA. 

Viés de medição ou conjunto de dados
Quando os dados são incorretamente rotulados, categorizados ou simplificados demais, isso resulta em viés de medição . O viés de medição pode ser introduzido quando uma pessoa comete um erro ao rotular os dados ou por erro de máquina. Uma característica, fator ou grupo pode ser super ou sub-representado em seu conjunto de dados. 

Vamos considerar um exemplo inofensivo: um sistema de reconhecimento de imagem para cães e gatos. Os dados de treinamento parecem bastante diretos — fotos de cães e gatos. Mas o conjunto de imagens inclui apenas fotos de cães pretos e gatos brancos ou marrons. Confrontada com uma foto de um cão branco, a IA o categoriza como um gato. Embora os dados de treinamento do mundo real raramente sejam tão claros e secos, os resultados podem ser tão surpreendentemente errados — com grandes consequências. 

Ilustração de dados de treinamento com fotos de seis cães pretos, quatro gatos brancos e dois gatos marrons alimentados em um algoritmo de aprendizado para um modelo preditivo. O modelo categoriza o cão branco como um “gato” com pontuação de confiança de 0,96.

Erro Tipo 1 vs. Erro Tipo 2
Pense em um banco usando IA para prever se um solicitante pagará um empréstimo. Se o sistema prevê que o solicitante será capaz de pagar o empréstimo, mas não o faz, é um falso positivo ou erro tipo 1. Se o sistema prevê que o solicitante não será capaz de pagar o empréstimo, mas o faz, é um falso negativo ou erro tipo 2. Os bancos querem conceder empréstimos a pessoas que eles têm certeza de que podem pagá-los. Para minimizar o risco, seu modelo é inclinado a erros tipo 2. Mesmo assim, os falsos negativos prejudicam os solicitantes que o sistema julga incorretamente como incapazes de pagar. 

Uma gangorra com um lado representando empréstimos de baixo risco para o banco e o outro lado representando empréstimos de alto risco, inclinando-se para alto risco.

Viés de Associação

Dados que são rotulados de acordo com estereótipos são um exemplo de viés de associação. Pesquise na maioria dos varejistas on-line por "brinquedos para meninas" e você obterá uma variedade infinita de brinquedos de cozinha, bonecas, princesas e rosa. Pesquise "brinquedos para meninos" e você verá bonecos de ação de super-heróis, conjuntos de construção e videogames. 

Viés de confirmação 

O viés de confirmação rotula os dados com base em ideias preconcebidas. As recomendações que você vê quando faz compras on-line refletem seus hábitos de compra, mas os dados que influenciam essas compras já refletem o que as pessoas veem e escolhem comprar em primeiro lugar. Você pode ver como os sistemas de recomendação reforçam estereótipos. Se os super-heróis não aparecem na seção "brinquedos para meninas" de um site, é improvável que um comprador saiba que eles estão em outro lugar no site, muito menos os compre.

Viés de automação 

O viés da automação impõe os valores de um sistema sobre os outros. Tomemos, por exemplo, um concurso de beleza julgado pela IA em 2016. O objetivo era declarar as mulheres mais bonitas com alguma noção de objetividade. Mas a IA em questão foi treinada principalmente em imagens de mulheres brancas e sua definição aprendida de "beleza" não incluía características mais comuns em pessoas de cor. Como resultado, a IA escolheu principalmente vencedores brancos, traduzindo um viés nos dados de treinamento em resultados do mundo real.

O viés da automação não se limita à IA. Veja a história da fotografia colorida. A partir de meados da década de 1950, a Kodak forneceu aos laboratórios fotográficos que revelavam seus filmes uma imagem de uma funcionária de pele clara chamada Shirley Page, que era usada para calibrar tons de pele, sombras e luz. Embora diferentes modelos tenham sido usados ​​ao longo do tempo, as imagens ficaram conhecidas como "cartões Shirley". O tom de pele de Shirley, independentemente de quem ela era (e ela era sempre branca no início), era considerado padrão. Como Lorna Roth, professora de mídia na Universidade Concordia do Canadá, disse à NPR , quando os cartões foram criados pela primeira vez, "as pessoas que estavam comprando câmeras eram, em sua maioria, caucasianas. E então acho que eles não viam a necessidade de o mercado se expandir para uma gama mais ampla de tons de pele". Na década de 1970, eles começaram a testar em uma variedade de tons de pele e fizeram cartões Shirley multirraciais.

Preconceito social 

O preconceito social reproduz os resultados do preconceito passado em relação a grupos historicamente marginalizados. Considere o redlining. Na década de 1930, uma política federal de habitação codificou por cores certos bairros em termos de desejabilidade. Os marcados em vermelho eram considerados perigosos. Os bancos frequentemente negavam acesso a empréstimos imobiliários de baixo custo a grupos minoritários residentes desses bairros marcados em vermelho. Até hoje, o redlining influenciou a composição racial e econômica de certos códigos postais, de modo que os códigos postais podem ser um proxy para raça. Se você incluir códigos postais como um ponto de dados em seu modelo, dependendo do caso de uso, você pode inadvertidamente incorporar raça como um fator na tomada de decisão do seu algoritmo. Lembre-se de que também é ilegal nos EUA usar categorias protegidas como idade, raça ou gênero na tomada de muitas decisões financeiras. 

Sobrevivência ou Viés de Sobrevivência

Às vezes, um algoritmo foca nos resultados daqueles que foram selecionados, ou que sobreviveram a um certo processo, às custas daqueles que foram excluídos. Vamos dar uma olhada nas práticas de contratação. Imagine que você é o diretor de contratação de uma empresa, e quer descobrir se deve recrutar de uma universidade específica. Você olha para os funcionários atuais contratados de tal e tal universidade. Mas e os candidatos que não foram contratados daquela universidade, ou que foram contratados e posteriormente dispensados? Você vê o sucesso apenas daqueles que "sobreviveram". 

Infográfico representando viés de sobrevivência no recrutamento universitário. Candidatos de três universidades passam pelo primeiro funil, e apenas candidatos que não deixaram a empresa passam pelo segundo. O grupo final não é representativo de recrutas dessas três universidades.

Viés de interação

Humanos criam viés de interação quando interagem com ou intencionalmente tentam influenciar sistemas de IA e criam resultados tendenciosos. Um exemplo disso é quando as pessoas intencionalmente tentam ensinar linguagem chula aos chatbots. 

Como o preconceito entra no sistema?
Você sabe que o preconceito pode entrar em um sistema de IA por meio dos criadores de um produto, por meio de dados de treinamento (ou falta de informações sobre todas as fontes que contribuem para um conjunto de dados) ou do contexto social no qual uma IA é implantada.

Suposições

Antes de alguém começar a construir um determinado sistema, eles frequentemente fazem suposições sobre o que devem construir, para quem devem construir e como ele deve funcionar, incluindo que tipo de dados coletar de quem. Isso não significa que os criadores de um sistema tenham más intenções, mas como humanos, nem sempre podemos entender as experiências de todos os outros ou prever como um determinado sistema impactará os outros. Podemos tentar limitar nossas próprias suposições de entrar em um produto incluindo diversas partes interessadas e participantes em nossos processos de pesquisa e design desde o início. Também devemos nos esforçar para ter equipes diversas trabalhando em sistemas de IA. 

Dados de treinamento

Os modelos de IA precisam de dados de treinamento, e é fácil introduzir viés com o conjunto de dados. Se uma empresa historicamente contrata das mesmas universidades, mesmos programas ou ao longo das mesmas linhas de gênero, um sistema de IA de contratação aprenderá que esses são os melhores candidatos. O sistema não recomendará candidatos que não correspondam a esses critérios.

Modelo

Os fatores que você usa para treinar um modelo de IA, como raça, gênero ou idade, podem resultar em recomendações ou previsões tendenciosas contra certos grupos definidos por essas características. Você também precisa estar atento a fatores que funcionam como proxies para essas características. O primeiro nome de alguém, por exemplo, pode ser um proxy para gênero, raça ou país de origem. Por esse motivo, os produtos Einstein não usam nomes como fatores em seu modelo Lead and Opportunity Scoring.

Einstein para Sales Lead e Opportunity Scoring. Uma caixa de diálogo avisa que “O código postal tem uma alta correlação com a raça e pode estar adicionando viés ao modelo preditivo desta história.”

Intervenção humana (ou falta dela)

Editar dados de treinamento impacta diretamente como o modelo se comporta e pode adicionar ou remover viés. Podemos remover dados de baixa qualidade ou pontos de dados super-representados, adicionar rótulos ou editar categorias ou excluir fatores específicos, como idade e raça. Também podemos deixar o modelo como está, o que, dependendo das circunstâncias, pode deixar espaço para viés.

As partes interessadas em um sistema de IA devem ter a opção de dar feedback sobre suas recomendações. Isso pode ser implícito (digamos, o sistema recomenda um livro que o cliente pode gostar e o cliente não o compra) ou explícito (digamos, o cliente dá um polegar para cima para uma recomendação). Esse feedback treina o modelo para fazer mais ou menos do que ele acabou de fazer. De acordo com o GDPR, os cidadãos da UE também devem ser capazes de corrigir informações incorretas que uma empresa tem sobre eles e pedir que essa empresa exclua seus dados. Mesmo que não seja exigido por lei, essa é a melhor prática, pois garante que sua IA esteja fazendo recomendações com base em dados precisos e garantindo a confiança do cliente.

A IA pode ampliar o preconceito
Treinar modelos de IA com base em conjuntos de dados tendenciosos frequentemente amplifica esses vieses. Em um exemplo , um conjunto de dados de fotos tinha 33% mais mulheres do que homens em fotos envolvendo culinária, mas o algoritmo amplificou esse viés para 68%. Para saber mais, veja a postagem do blog na seção de recursos.
### Recognize Bias in Artificial Intelligence
Gerenciar riscos de preconceito
Discutimos os diferentes tipos de viés a serem considerados ao trabalhar com IA. Agora, a parte difícil: como prevenir ou gerenciar os riscos que esses vieses criam. Você não pode magicamente desviá-los de seus dados de treinamento. Remover a exclusão é um problema social e técnico: você pode tomar precauções como uma equipe em como planeja e executa seu produto, além de modificar seus dados. 

Realizar pré-mortes
Como discutimos na primeira unidade, criar um produto de forma responsável começa com a construção de uma cultura ética. Uma maneira de fazer isso é incorporando premortems ao seu fluxo de trabalho. 

Um premortem é o oposto de um post-mortem — é uma oportunidade de pegar o "o que deu errado" antes que aconteça. Frequentemente, os membros da equipe podem hesitar em compartilhar reservas na fase de planejamento de um projeto . Em uma área sensível como a IA, é fundamental que você e sua equipe sejam abertos sobre quaisquer dúvidas que possam ter e estejam dispostos a ficar desconfortáveis. Realizar tal reunião pode moderar o desejo de jogar a cautela ao vento no entusiasmo inicial sobre um projeto, definindo expectativas medidas e realistas. 

Identifique fatores excluídos ou super-representados em seu conjunto de dados
Considere os fatores sociais e culturais profundos que são refletidos em seu conjunto de dados. Como detalhamos na unidade anterior, qualquer viés no nível do seu conjunto de dados pode impactar o sistema de recomendação da sua IA e pode resultar na super ou sub-representação de um grupo.

De uma perspectiva técnica, aqui estão algumas maneiras de abordar o viés em seus dados. Essas técnicas não são de forma alguma abrangentes.

O que : Padrões estatísticos que se aplicam à maioria podem ser inválidos dentro de um grupo minoritário.

Como : Considere criar algoritmos diferentes para grupos diferentes, em vez de uma abordagem única para todos.

O quê : Pessoas são excluídas do seu conjunto de dados, e essa exclusão tem um impacto sobre seus usuários. Contexto e cultura importam, mas pode ser impossível ver os efeitos nos dados.

Como : Procure o que os pesquisadores chamam de desconhecidos desconhecidos, erros que acontecem quando um modelo está altamente confiante sobre uma previsão que está realmente errada. Desconhecidos desconhecidos estão em contraste com desconhecidos conhecidos, previsões incorretas que o modelo faz com baixa confiança. Semelhante a quando um modelo gera conteúdo, ele pode produzir informações completamente não factuais para sua solicitação.

Avalie regularmente seus dados de treinamento
Como dissemos antes, desenvolver um sistema de IA começa no nível dos seus dados de treinamento. Você deve ser escrupuloso sobre como abordar problemas de qualidade de dados o mais cedo possível no processo. Certifique-se de abordar extremos, duplicatas, outliers e redundância no CRM Analytics ou outras ferramentas de preparação de dados.

Antes de lançar seus modelos, certifique-se de executar testes de pré-lançamento para que seu sistema não faça previsões ou julgamentos tendenciosos e impacte as pessoas no mundo real. Certifique-se de que eles foram testados para que não causem danos. Você quer ser capaz de contabilizar seu produto funcionando em diferentes comunidades para que você não tenha surpresas no lançamento. 

Depois de liberar um modelo, desenvolva um sistema para verificar periodicamente os dados dos quais seus algoritmos estão aprendendo e as recomendações que seu sistema está fazendo. Pense em seus dados como tendo uma meia-vida — eles não funcionarão para todos indefinidamente. No lado técnico, quanto mais dados entram em um sistema, mais um algoritmo aprende. Isso pode levar o sistema a identificar e combinar padrões que aqueles que estão desenvolvendo o produto não previram ou não queriam. 

No lado social, os valores culturais mudam ao longo do tempo. A saída dos seus algoritmos pode não mais se adequar aos sistemas de valores das comunidades que ele atende. Duas maneiras de lidar com esses desafios incluem processos de revisão da comunidade pagos para corrigir a supervisão e criar mecanismos em seu produto para que indivíduos e usuários optem por não participar ou corrijam dados sobre si mesmos. Os processos de revisão da comunidade devem incluir pessoas das comunidades que podem ser impactadas pelo sistema algorítmico que você está desenvolvendo. Você também deve realizar sessões com as pessoas que implementarão, gerenciarão e usarão o sistema para atingir as metas de sua organização. Acesse nossos UX Research Basics para saber mais sobre os métodos que você pode usar para conduzir processos de revisão da comunidade, bem como conduzir pesquisas de usuários para entender os contextos em que sua ferramenta será usada.

Conclusão
A IA pode ser uma força para o bem, potencialmente detectando tumores que os humanos não conseguem e Alzheimer antes que a família possa ou preservando línguas indígenas . Ao longo deste módulo, mostramos o poder dos sistemas de IA, mas também sua opacidade. Se quisermos que a IA beneficie a sociedade mais do que a prejudique, temos que reconhecer os riscos e tomar medidas para garantir que os sistemas de IA sejam projetados, desenvolvidos e usados ​​de forma responsável.

Como tecnólogos, mesmo quando somos conscientes e deliberados em nossa abordagem, haverá surpresas ao longo do caminho. Nem sempre podemos prever as interações entre conjuntos de dados, modelos e seu contexto cultural. Os conjuntos de dados geralmente contêm vieses dos quais não temos conhecimento, e é nossa responsabilidade avaliar e avaliar os dados de treinamento e as previsões de nossos modelos para garantir que eles não produzam resultados prejudiciais.

Desenvolver sistemas éticos de IA é um processo sociotécnico. Olhe para ele não apenas em termos de sua implementação técnica, mas também pela maneira como ele é desenvolvido entre equipes e os contextos sociais em que será usado. Além disso, avalie quem está envolvido no processo — como gênero, raça, etnia e idade são representados? As pessoas que constroem produtos de IA e o preconceito gerado por esses sistemas estão interconectados.

Para concretizar uma IA segura e socialmente benéfica, precisamos lembrar que os humanos estão no centro dela. A IA é uma ferramenta e nós escolhemos como usá-la. Independentemente do papel de alguém, suas pequenas decisões podem ter consequências sérias e duradouras. Na Salesforce, acreditamos firmemente que podemos fazer bem e fazer o bem. Você pode lucrar sem prejudicar os outros e, de fato, causar um impacto positivo no processo. 
### Remove Bias from Your Data and Algorithms
Gerenciar riscos de preconceito
Discutimos os diferentes tipos de viés a serem considerados ao trabalhar com IA. Agora, a parte difícil: como prevenir ou gerenciar os riscos que esses vieses criam. Você não pode magicamente desviá-los de seus dados de treinamento. Remover a exclusão é um problema social e técnico: você pode tomar precauções como uma equipe em como planeja e executa seu produto, além de modificar seus dados. 

Realizar pré-mortes
Como discutimos na primeira unidade, criar um produto de forma responsável começa com a construção de uma cultura ética. Uma maneira de fazer isso é incorporando premortems ao seu fluxo de trabalho. 

Um premortem é o oposto de um post-mortem — é uma oportunidade de pegar o "o que deu errado" antes que aconteça. Frequentemente, os membros da equipe podem hesitar em compartilhar reservas na fase de planejamento de um projeto . Em uma área sensível como a IA, é fundamental que você e sua equipe sejam abertos sobre quaisquer dúvidas que possam ter e estejam dispostos a ficar desconfortáveis. Realizar tal reunião pode moderar o desejo de jogar a cautela ao vento no entusiasmo inicial sobre um projeto, definindo expectativas medidas e realistas. 

Identifique fatores excluídos ou super-representados em seu conjunto de dados
Considere os fatores sociais e culturais profundos que são refletidos em seu conjunto de dados. Como detalhamos na unidade anterior, qualquer viés no nível do seu conjunto de dados pode impactar o sistema de recomendação da sua IA e pode resultar na super ou sub-representação de um grupo.

De uma perspectiva técnica, aqui estão algumas maneiras de abordar o viés em seus dados. Essas técnicas não são de forma alguma abrangentes.

O que : Padrões estatísticos que se aplicam à maioria podem ser inválidos dentro de um grupo minoritário.

Como : Considere criar algoritmos diferentes para grupos diferentes, em vez de uma abordagem única para todos.

O quê : Pessoas são excluídas do seu conjunto de dados, e essa exclusão tem um impacto sobre seus usuários. Contexto e cultura importam, mas pode ser impossível ver os efeitos nos dados.

Como : Procure o que os pesquisadores chamam de desconhecidos desconhecidos, erros que acontecem quando um modelo está altamente confiante sobre uma previsão que está realmente errada. Desconhecidos desconhecidos estão em contraste com desconhecidos conhecidos, previsões incorretas que o modelo faz com baixa confiança. Semelhante a quando um modelo gera conteúdo, ele pode produzir informações completamente não factuais para sua solicitação.

Avalie regularmente seus dados de treinamento
Como dissemos antes, desenvolver um sistema de IA começa no nível dos seus dados de treinamento. Você deve ser escrupuloso sobre como abordar problemas de qualidade de dados o mais cedo possível no processo. Certifique-se de abordar extremos, duplicatas, outliers e redundância no CRM Analytics ou outras ferramentas de preparação de dados.

Antes de lançar seus modelos, certifique-se de executar testes de pré-lançamento para que seu sistema não faça previsões ou julgamentos tendenciosos e impacte as pessoas no mundo real. Certifique-se de que eles foram testados para que não causem danos. Você quer ser capaz de contabilizar seu produto funcionando em diferentes comunidades para que você não tenha surpresas no lançamento. 

Depois de liberar um modelo, desenvolva um sistema para verificar periodicamente os dados dos quais seus algoritmos estão aprendendo e as recomendações que seu sistema está fazendo. Pense em seus dados como tendo uma meia-vida — eles não funcionarão para todos indefinidamente. No lado técnico, quanto mais dados entram em um sistema, mais um algoritmo aprende. Isso pode levar o sistema a identificar e combinar padrões que aqueles que estão desenvolvendo o produto não previram ou não queriam. 

No lado social, os valores culturais mudam ao longo do tempo. A saída dos seus algoritmos pode não mais se adequar aos sistemas de valores das comunidades que ele atende. Duas maneiras de lidar com esses desafios incluem processos de revisão da comunidade pagos para corrigir a supervisão e criar mecanismos em seu produto para que indivíduos e usuários optem por não participar ou corrijam dados sobre si mesmos. Os processos de revisão da comunidade devem incluir pessoas das comunidades que podem ser impactadas pelo sistema algorítmico que você está desenvolvendo. Você também deve realizar sessões com as pessoas que implementarão, gerenciarão e usarão o sistema para atingir as metas de sua organização. Acesse nossos UX Research Basics para saber mais sobre os métodos que você pode usar para conduzir processos de revisão da comunidade, bem como conduzir pesquisas de usuários para entender os contextos em que sua ferramenta será usada.

Conclusão
A IA pode ser uma força para o bem, potencialmente detectando tumores que os humanos não conseguem e Alzheimer antes que a família possa ou preservando línguas indígenas . Ao longo deste módulo, mostramos o poder dos sistemas de IA, mas também sua opacidade. Se quisermos que a IA beneficie a sociedade mais do que a prejudique, temos que reconhecer os riscos e tomar medidas para garantir que os sistemas de IA sejam projetados, desenvolvidos e usados ​​de forma responsável.

Como tecnólogos, mesmo quando somos conscientes e deliberados em nossa abordagem, haverá surpresas ao longo do caminho. Nem sempre podemos prever as interações entre conjuntos de dados, modelos e seu contexto cultural. Os conjuntos de dados geralmente contêm vieses dos quais não temos conhecimento, e é nossa responsabilidade avaliar e avaliar os dados de treinamento e as previsões de nossos modelos para garantir que eles não produzam resultados prejudiciais.

Desenvolver sistemas éticos de IA é um processo sociotécnico. Olhe para ele não apenas em termos de sua implementação técnica, mas também pela maneira como ele é desenvolvido entre equipes e os contextos sociais em que será usado. Além disso, avalie quem está envolvido no processo — como gênero, raça, etnia e idade são representados? As pessoas que constroem produtos de IA e o preconceito gerado por esses sistemas estão interconectados.

Para concretizar uma IA segura e socialmente benéfica, precisamos lembrar que os humanos estão no centro dela. A IA é uma ferramenta e nós escolhemos como usá-la. Independentemente do papel de alguém, suas pequenas decisões podem ter consequências sérias e duradouras. Na Salesforce, acreditamos firmemente que podemos fazer bem e fazer o bem. Você pode lucrar sem prejudicar os outros e, de fato, causar um impacto positivo no processo. 
### Create Responsible Generative AI
IA generativa, um novo tipo de inteligência artificial
Até recentemente, a maioria das pessoas que discutiam IA falava sobre IA preditiva. Esse tipo de inteligência artificial se concentra em olhar para um conjunto existente de dados e fazer previsões limitadas sobre o que deve ser verdade, dadas as informações em mãos. Agora, há um novo jogador no campo — um tipo emergente de IA que é generativa, não preditiva. A principal diferença? Onde a IA preditiva analisa tendências, a IA generativa cria novos conteúdos.

A IA generativa (gen AI) ostenta uma impressionante variedade de capacidades — desde conversas em tempo real com bots que simulam efetivamente uma conversa com um agente de suporte ao vivo até aplicativos para profissionais de marketing, programadores e pioneiros criativos. Além disso, o momento cultural da gen AI tem feito com que os usuários se aglomerem para ver o que ela pode fazer. Isso significa que a maioria de nós provavelmente encontrará esses algoritmos em nossas vidas diárias, onde eles podem desempenhar um papel cada vez mais significativo.

Com toda tecnologia emergente vêm incógnitas. Seja abuso intencional ou viés acidental, a IA gen apresenta riscos que devem ser compreendidos e abordados para obter o máximo dessa tecnologia.

Conheça os riscos
Na Salesforce, focamos em projetar, desenvolver e distribuir tecnologias de forma responsável e confiável. Para fazer isso, antecipamos as consequências intencionais e não intencionais do que construímos.

Vamos rever alguns riscos potenciais para a IA gerada.

Precisão


Os modelos Gen AI são ótimos para fazer previsões. Os modelos Gen AI criam novos conteúdos reunindo toneladas de exemplos de coisas que se encaixam nas mesmas categorias. Mas, embora um modelo possa criar uma nova frase no estilo de um escritor famoso, não há como saber se a mesma frase é factualmente verdadeira. E isso pode ser um problema quando os usuários presumem que as previsões de uma IA são fatos verificados. Isso é um recurso e um bug. Ele dá aos modelos as capacidades criativas que capturaram a imaginação em seus primeiros dias. Mas é fácil confundir algo que parece correto com algo que é preciso para o mundo real. 

Viés e Toxicidade


Como as interações humanas podem envolver um grau de toxicidade — isto é, comportamento prejudicial como usar calúnias ou defender intolerância — a IA replica essa toxicidade quando não é ajustada para reconhecê-la e filtrá-la. Na verdade, ela pode até amplificar o viés que encontra, porque fazer previsões geralmente envolve descartar dados periféricos. Para uma IA, isso pode incluir comunidades sub-representadas. 

Privacidade e Segurança


Os dois recursos mais atraentes da Gen AI são sua capacidade de replicar o comportamento humano e a velocidade para fazer isso em uma escala massiva. Esses recursos oferecem possibilidades incríveis. E há uma desvantagem: é fácil explorar a tecnologia para causar enormes quantidades de dano muito rapidamente. Os modelos têm uma tendência a "vazar" seus dados de treinamento, expondo informações privadas sobre as pessoas representadas neles. E a gen AI pode até ser usada para criar e-mails de phishing confiáveis ​​ou replicar uma voz para contornar a segurança. 

Perturbação


Por causa do quanto a IA pode fazer, ela representa um risco para a sociedade, mesmo quando funciona como pretendido. Perturbações econômicas, mudanças de emprego e responsabilidade, e preocupações com a sustentabilidade do intenso poder de computação necessário para que os modelos operem, tudo isso tem implicações para os espaços que compartilhamos. 

Confiança: o resultado final
A confiança é o valor número 1 na Salesforce e é nossa estrela-guia à medida que construímos e implantamos aplicativos de IA gen. Para orientar esse trabalho, criamos um conjunto de  princípios para desenvolver IA generativa de forma responsável e ajudar outros a alavancar o potencial da tecnologia enquanto se protegem contra suas armadilhas.

Precisão: Gen AI, como outros modelos, faz previsões com base nos dados em que é treinada. Isso significa que ela precisa de bons dados para fornecer resultados precisos. E significa que as pessoas precisam estar cientes da chance de imprecisão ou incerteza na saída de uma IA.

Segurança: avaliações de viés, explicabilidade e robustez, juntamente com testes de estresse deliberados para resultados negativos, nos ajudam a manter os clientes seguros de perigos como toxicidade e dados enganosos. Também protegemos a privacidade de qualquer informação de identificação pessoal (PII) presente nos dados usados ​​para treinamento. E criamos barreiras de proteção para evitar danos adicionais (como publicar código em um sandbox em vez de enviar automaticamente para produção).

Honestidade:  Seus dados não são nosso produto. Ao coletar dados para treinar e avaliar nossos modelos, precisamos respeitar a procedência dos dados e garantir que temos consentimento para usar os dados (por exemplo, de código aberto, fornecidos pelo usuário). Também é importante notificar as pessoas quando elas estiverem usando ou falando com uma IA, com uma marca d'água ou isenção de responsabilidade, para que elas não confundam um chatbot bem ajustado com um agente humano.

Empoderamento:  Há alguns casos em que é melhor automatizar completamente os processos. Mas há outros casos em que a IA deve desempenhar um papel de apoio ao humano — ou em que o julgamento humano é necessário. Nosso objetivo é potencializar o que os humanos podem fazer desenvolvendo IA que aprimore ou simplifique seu trabalho e dê aos clientes ferramentas e recursos para entender a veracidade do conteúdo que eles criam. 

Sustentabilidade:  Quando se trata de modelos de IA, maior nem sempre significa melhor: Em alguns casos, modelos menores e mais bem treinados superam modelos maiores e menos treinados. Encontrar o equilíbrio certo entre poder algorítmico e sustentabilidade de longo prazo é uma parte fundamental para trazer a IA de geração para o nosso futuro compartilhado.

Diretrizes Governam a Ação da IA
Então, como é cumprir esses compromissos? Aqui estão algumas ações que a Salesforce está tomando.

Einstein Trust Layer:  Integramos o Einstein Trust Layer à plataforma Salesforce para ajudar a elevar a segurança da IA ​​de geração no Salesforce por meio de controles de dados e privacidade que são perfeitamente integrados à experiência do usuário final. Você pode aprender mais verificando o Einstein Trust Layer na Ajuda .

Decisões de design de produto: os usuários devem poder confiar que, ao usar a IA, obterão insights e assistência confiáveis ​​que os capacitarão a atender às suas necessidades sem expô-los ao risco de compartilhar algo impreciso ou enganoso. 

Nós construímos responsabilidade em nossos produtos. Examinamos tudo, desde a cor dos botões até as limitações nas próprias saídas para garantir que estamos fazendo todo o possível para proteger os clientes do risco sem sacrificar os recursos nos quais eles confiam para permanecer competitivos. 

Atrito consciente: os usuários devem sempre ter as informações de que precisam para tomar a melhor decisão para seu caso de uso. Ajudamos nossos usuários a se manterem à frente da curva com atrito não intrusivo, mas aplicado de forma consciente. Neste caso, "atrito" significa interromper o processo usual de conclusão de uma tarefa para incentivar a reflexão. Por exemplo, pop-ups de orientação no aplicativo para educar os usuários sobre viés ou sinalizar toxicidade detectada e pedir aos agentes de atendimento ao cliente que revisem a resposta cuidadosamente antes de enviar.

Red teaming: Empregamos o red teaming, um processo que envolve tentar intencionalmente encontrar vulnerabilidades em um sistema, antecipando e testando como os usuários podem usá-lo e usá-lo incorretamente, para garantir que nossos produtos de IA de geração resistam à pressão. Saiba mais sobre como o Salesforce cria confiança em nossos produtos com o Einstein Trust Layer no Trailhead .

Uma maneira de testar nossos produtos é realizando “ataques de injeção de prompt” preventivos, elaborando prompts projetados especificamente para fazer um modelo de IA ignorar instruções ou limites previamente estabelecidos. Antecipar ameaças reais de segurança cibernética como essas é essencial para refinar o modelo para resistir a ataques reais.

Política de uso aceitável: como a IA toca em tantos aplicativos diferentes, temos políticas específicas para nossos produtos de IA. Isso nos permite definir diretrizes de uso aceitável de forma transparente que garantem a confiança para nossos clientes e usuários finais. Essa abordagem não é nenhuma novidade: a Salesforce já tinha políticas de IA projetadas para proteger os usuários, incluindo uma proibição de reconhecimento facial e bots se passando por humanos. 

No momento, estamos atualizando nossas diretrizes de IA existentes para levar em conta a IA de geração, para que os clientes possam continuar confiando em nossa tecnologia. Com nossas regras atualizadas, qualquer um pode ver se seu caso de uso é suportado, pois oferecemos produtos e recursos de IA ainda mais avançados. Você pode saber mais verificando nossa Política de Uso Aceitável . 

Navegue na jornada para a frente
A Gen AI muda o jogo de como as pessoas e as empresas podem trabalhar juntas. Embora não tenhamos todas as respostas, sugerimos algumas práticas recomendadas.

Colaborar


Parcerias multifuncionais – dentro de empresas e entre instituições públicas e privadas – são essenciais para impulsionar o progresso responsável. Nossas equipes estão participando ativamente de comitês e iniciativas externas, como o National AI Advisory Committee (NAIAC) e a  estrutura de gerenciamento de risco do NIST para contribuir com o impulso de toda a indústria para criar uma IA gen mais confiável.

Incluir Perspectivas Diversas


Ao longo do ciclo de vida do produto, perspectivas diversas fornecem os insights abrangentes necessários para antecipar riscos de forma eficaz e desenvolver soluções para eles. Exercícios como  a varredura de consequências podem ajudar você a garantir que seus produtos incorporem vozes essenciais na conversa sobre onde a IA de geração está hoje e para onde levá-la amanhã.

Mesmo a IA mais avançada não consegue prever como essa tecnologia moldará o futuro do trabalho, do comércio e, aparentemente, de tudo o mais. Mas, trabalhando juntos, podemos garantir que os valores centrados no ser humano criem uma base de confiança a partir da qual construir um futuro mais eficiente e escalável.

## Gain Insight and Improve Outcomes with Einstein Discovery
### Einstein Discovery: Quick Look
### Einstein Discovery Basics
### Ethical Model Development with Einstein Discovery: Quick Look
### Einstein Discovery for Reports: Quick Look
### Einstein Prediction Service